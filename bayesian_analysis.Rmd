---
title: "Analysis Roulette Gamblingte Study"
author: "Henrik"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  html_document:
    toc: true
    toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(dpi=200, out.width="70%", fig.asp = 0.618,
                      fig.width=4, fig.align = "center")
options(width = 110)
options("dplyr.summarise.inform" = FALSE)
options(pillar.sigfig = 3)
options(mc.cores = parallel::detectCores()) # fitting uses multicore by default
```

# Preparation

```{r, message=FALSE, warning=FALSE, results='hide'}
library("tidyverse")
#library("tidylog")
theme_set(theme_bw() + 
            theme(panel.grid.major.x = element_blank(), 
                  panel.grid.minor.x = element_blank()))
library("brms")
library("tidybayes")
library("BayesFactor")
library("binom")
library("emmeans")
par_labels <- c("Gamble at all?", 
                "Gamble everything?", 
               "Proportion bet?")
cond_labels <- c("Normal", "Slowed down")
ylabel <- "Gambling speed"
```


```{r, message=FALSE, warning=FALSE}
participants <- read_csv("data/participants_anon.csv") %>% 
  mutate(pptid = factor(ppt_id))
```

Total participants (NULL = did not pass captchas):

```{r}
table(participants$exp_cond)
```

Participants that passed the captchas and finished the study:

```{r}
participants_red <- participants %>% 
  filter(progress == "outro", exp_cond != "NULL") %>% 
  mutate(
    exp_cond = factor(
      exp_cond, 
      levels = c("normal", "delay"),
      labels = cond_labels
    ),
    bonus = as.numeric(bonus), 
    bet_count = as.numeric(bet_count)
  ) %>% 
  droplevels()

part2 <- participants_red %>% 
  select(ppt_id, exp_cond, bonus, bet_count)

table(participants_red$exp_cond)
```

```{r, message=FALSE, warning=FALSE, include=FALSE}
bets <- read_csv("data/bets_anon.csv")
bets2 <- bets %>% 
  group_by(ppt_id) %>% 
  summarise(amount = sum(abs(total_amt_bet)), 
            average = mean(abs(total_amt_bet)),
            total_win = sum(win)) 

part2 <- left_join(part2, bets2) %>% 
  mutate(amount = if_else(is.na(amount), 0, amount),
         total_win = if_else(is.na(total_win), 0, total_win)) %>% 
  mutate(new_prop = amount/ (3 + total_win)) %>% 
  mutate(new_prop = if_else(new_prop > 1, 1, new_prop)) ## necessary: few values are just above 1
```

```{r, message=FALSE, warning=FALSE}

covariates <- read_csv("data/questions_anon.csv") %>% 
  mutate(across(starts_with("PGSI"), ~case_when(
    . == "Never" ~ 0,
    . == "Sometimes" ~ 1,
    . == "MostTime" ~ 2, 
    . == "Always" ~ 3, 
    TRUE ~ NA_real_))) %>% 
  mutate(pptid = factor(ppt_id)) 
# map(covariates[,-1], ~sort(unique(.)))
## PGSI
# 0 = never
# 1 = sometimes
# 2 = most often / often
# 3 = always
pgsi <- covariates %>% 
  #select(-starts_with("Motives")) %>% 
  pivot_longer(starts_with("PGSI")) %>% 
  group_by(ppt_id) %>% 
  summarise(pgsi = sum(value))

part2 <- left_join(part2, pgsi) %>% 
  mutate(pgsi_c = pgsi - mean(pgsi))
```


# Payed bonus

Overall bonus paid out was:

```{r}
round(mean(part2$bonus), 2)
```

If we split bonus by whether or not participants played, we see that playing leads to a slightly higher payout:

```{r}
part2 %>% 
  mutate(bet_at_all = ifelse(bet_count == 0, "no bet", "bet")) %>% 
  group_by(bet_at_all) %>% 
  summarise(mean_bonus = mean(bonus), 
            sd_bonus = sd(bonus), 
            se_bonus = sd(bonus)/sqrt(n()), 
            max_bonus = max(bonus),
            min_bonus = min(bonus))
```


# Distribution of Proportion Bet

Our main DV clearly does not look normally distributed.

```{r}
part2 %>% 
  ggplot(aes(new_prop)) +
  geom_histogram(binwidth = 0.025)
```

```{r}
part2 %>% 
  summarise(gamble_at_all = 1 - mean(new_prop  == 0), 
            gamble_everything = mean(new_prop[new_prop != 0] == 1), 
            proportion_bet_rest = mean(new_prop[!(new_prop %in% c(0, 1))]))


```

Binomial confidence or credibility intervals for the probability to gamble at all:

```{r}
binom.confint(nrow(part2) - sum(part2$new_prop == 0), nrow(part2))
```



# Model 1: Zero-One Inflated Beta Regression on Proportion Bet

We use a custom parameterization of a zero-one-inflated beta-regression model (see also [here](https://vuorre.netlify.com/post/2019/02/18/analyze-analog-scale-ratings-with-zero-one-inflated-beta-models/)). The likelihood of the model is given by:

$$\begin{align}
f(y) &= (1 - g) & & \text{if } y = 0 \\
f(y) &= g \times e & & \text{if } y = 1 \\
f(y) &= g \times (1 - e) \times \text{Beta}(a,b) & & \text{if } y \notin \{0, 1\} \\
a &= \mu \times \phi \\
b &= (1-\mu) \times \phi
\end{align}$$

Where $1 - g$ is the zero inflation probability, `zipp` is $g$ and reflects the probability to gamble, $e$ is the conditional one-inflation probability (`coi`) or conditional probability to gamble everything (i.e., conditional probability to have a value of one, if one gambles), $\mu$ is the mean of the beta distribution (`Intercept`), and $\phi$ is the precision of the beta distribution (`phi`). As we use `Stan` for modelling, we need to model on the real line and need appropriate link functions. For `\phi` the link is log (inverse is `exp()`), for all other parameters it is logit (inverse is `plogis()`).

We fit this model and add experimental condition as a factor to the three main model parameters (i.e., only the precision parameter is fixed across conditions). The following table provides the overview of the model and all model parameters and show good convergence.

```{r, eval=TRUE, include=FALSE}
zoib2 <- custom_family(
  "zoib2", dpars = c("mu", "phi", "zipp", "coi"),
  links = c("logit", "log", "logit", "logit"), lb = c(NA, 0, NA, NA),
  type = "real"
)

stan_funs <- "
/* zero-one-inflated beta log-PDF of a single response 
   * Args: 
   *   y: response value 
   *   mu: mean parameter of the beta part
   *   phi: precision parameter of the beta part
   *   zipp: zero-inflation probability parameter
   *   coi: conditional one-inflation probability
   * Returns:  
   *   a scalar to be added to the log posterior 
   */ 
   real zoib2_lpdf(real y, real mu, real phi,
                                    real zipp, real coi) {
     row_vector[2] shape = [mu * phi, (1 - mu) * phi]; 
     if (y == 0) { 
       return bernoulli_lpmf(0 | zipp); 
     } else if (y == 1) {
       return bernoulli_lpmf(1 | zipp) + bernoulli_lpmf(1 | coi);
     } else { 
       return bernoulli_lpmf(1 | zipp) + bernoulli_lpmf(0 | coi) + beta_lpdf(y | shape[1], shape[2]);
     } 
   }
"
stanvars <- stanvar(scode = stan_funs, block = "functions")


zoib_model <- bf(
  new_prop ~ exp_cond,
  phi ~ 1,
  zipp ~ exp_cond,
  coi ~ exp_cond, 
  family = zoib2
)

np <- prior_string("target += logistic_lpdf(Intercept_zipp | 0, 1);", check = FALSE)

# make_stancode(zoib_model, data = part2, stanvars = stanvars, prior = np)
# tmp <- make_standata(zoib_model, data = part2, stanvars = stanvars) 
# str(tmp)
# tmp$Y
tmp_model_filename <- "model_fits/model_zoib.rda"
if (file.exists(tmp_model_filename)) {
  load(tmp_model_filename)
} else {
  mzoib <- brm(formula = zoib_model, data = part2, 
               stanvars = stanvars, prior = np, 
               iter = 26000, warmup = 1000, chains = 4) ## 100,000 post-warmup
save(mzoib, file = tmp_model_filename, compress = "xz")
}

```


```{r}
summary(mzoib)
```

As a visual convergence check, we plot the density and trace plots for the four intercept parameters representing the normal speed condition or the overall mean (for `phi`).

```{r, fig.asp=1.2}
plot(mzoib, pars = "Intercept")
```

We can also plot the three parameters showing the difference distribution of the slowed down condition from the normal speed condition. These differences are given on the logit scale.

```{r, fig.asp=1.0}
plot(mzoib, pars = "_condSloweddown")
```


```{r, include=FALSE}
get_variables(mzoib)
```


The model does not have any obvious problems, even without priors for the condition specific effects.

## Posterior Predictive Checks

As expected the synthetic data generated from the model looks a lot like the actual data. This suggests that the model is adequate for the data.

```{r}
## we need to create a custom RNG function for this case
posterior_predict_zoib2 <- function(i, prep, ...) {
  zi <- brms:::get_dpar(prep, "zipp", i)
  coi <- brms:::get_dpar(prep, "coi", i)
  mu <- brms:::get_dpar(prep, "mu", i = i)
  phi <- brms:::get_dpar(prep, "phi", i = i)
  hu <- runif(prep$nsamples, 0, 1)
  hu2 <- runif(prep$nsamples, 0, 1)
  #one_or_zero <- runif(prep$nsamples, 0, 1)
  ifelse(hu > zi, 0, 
    ifelse(hu2 < coi, 1, 
           rbeta(prep$nsamples, shape1 = mu * phi, shape2 = (1 - mu) * phi)
  ))
}

```

```{r, fig.width=6, out.width="100%"}
pp1 <- pp_check(mzoib, type = "hist", binwidth = 0.025, nsamples = 11) +
  coord_cartesian(ylim = c(0, 350)) +
  theme(legend.position = "none")
pp1
ggsave(plot = pp1, filename = "figures/ppp.png", width = 8, height = 6)

```

## Model Estimates

We first give the table showing the posterior means and 95% CIs.

```{r, warning=FALSE}

lpars <- mzoib %>% 
  gather_draws(`b_.*`, regex = TRUE) %>% 
  filter(.variable != "b_phi_Intercept") %>% 
  mutate(type = case_when(
    str_detect(.variable, "Intercept") ~ "Intercept",
    str_detect(.variable, "_condSloweddown") ~ "slowed"
  )) %>%  
  mutate(parameter = case_when(
    str_detect(.variable, "zipp") ~ "g",
    str_detect(.variable, "coi") ~ "f",
    TRUE ~ "mu"
  ))

condpars <- lpars %>% 
  pivot_wider(id_cols = c(.chain:.draw, parameter, parameter), 
              names_from = type, values_from = .value) %>% 
  mutate(Normal = Intercept, 
         SlowedDown = Intercept + slowed) %>% 
  select(-Intercept, -slowed) %>% 
  pivot_longer(cols = c(Normal, SlowedDown), 
               names_to = "condition", values_to = "estimate") %>% 
  mutate(estimate = plogis(estimate)) %>% 
  mutate(parameter = factor(
    x = parameter, 
    levels = c("g", "f", "mu"), 
    labels = par_labels))

condpars %>% 
  group_by(parameter, condition) %>% 
  mean_qi()

```

For the zero-one inflated components, we can compare the model estimates with the data. Not unsurprisingly, they match quite well. 

```{r}
part2 %>% 
  group_by(exp_cond) %>% 
  summarise(gamble_at_all = 1 - mean(new_prop  == 0), 
            gamble_everything = mean(new_prop[new_prop != 0] == 1))
```

The following is the main results figure on the level of condition.


```{r, out.width="100%", fig.width = 9.5}
## plots in which CIs sizes are relative to each other.
tmp <- condpars %>% 
  group_by(parameter, condition) %>% 
  mean_qi()
tmp <- tmp %>% 
  mutate(width = .upper - .lower) %>% 
  summarise(mwidth = mean(width))
pr1 <- condpars %>% 
  ggplot(aes(x = estimate, y = condition)) +
  #stat_histintervalh(breaks = 40) +
  stat_halfeye() +
  facet_wrap("parameter", scales = "free_x") +
  xlab("Probability/Proportion") + ylab(ylabel) +
  scale_x_continuous(labels = scales::percent_format(1))
tmp2 <- pr1$data %>% 
  group_by(parameter) %>% 
  summarise(min = min(estimate), 
            max = max(estimate)) %>% 
  mutate(fwidth = max - min) %>% 
  mutate(newmin = min, 
         newmax = max, 
         mid = min + fwidth * 0.5)
tmp2$fac <- tmp$mwidth/tmp2$fwidth
tmp2$ciwdith <- tmp$mwidth
tmp2 <- tmp2 %>% 
  mutate(newwidth = ciwdith * 1/min(fac)) %>% 
  mutate(newmin = mid - newwidth/2,
         newmax = mid + newwidth/2)
tmpl <- tmp2 %>% 
  select(parameter, newmin, newmax) %>% 
  pivot_longer(cols = c(newmin, newmax), 
               names_to = "bounds", values_to = "estimate") %>% 
  mutate(condition = NA)
pr1 <- pr1 +
  geom_blank(data = tmpl)

pr1 + theme_bw(base_size = 17)
ggsave(pr1  + theme_bw(base_size = 17), filename = "figures/res_speed_a.png", 
       width = 9.5, height = 4.5)
ggsave(pr1  + theme_bw(base_size = 17) + theme(panel.grid.minor.x = element_blank()), 
       filename = "figures/res_speed_b.png", 
       width = 9.5, height = 4.5)
ggsave(pr1  + theme_bw(base_size = 17) + theme(
  panel.grid.minor.x = element_blank(), 
  panel.grid.major.x = element_blank()), 
       filename = "figures/res_speed_c.png", 
       width = 9.5, height = 4.5)
pr1use <- pr1  + theme_bw(base_size = 17) + theme(
  panel.grid.minor.x = element_blank(), 
  panel.grid.major.x = element_blank())
```


## Difference distribution

We can also focus and look at the difference distributions.

```{r}
comppars <- condpars %>% 
  group_by(parameter) %>% 
  compare_levels(estimate, by = condition) %>% 
  mutate(condition = factor(
    x = condition, labels = "          "
  ))
comppars %>% 
  group_by(parameter, condition) %>% 
  mean_qi()
```

Same as a figure.


```{r, out.width="100%", fig.width = 9.5, fig.asp=0.4}

pr2 <- comppars %>% 
  ggplot(aes(x = estimate, y = condition)) +
  geom_vline(xintercept = 0, color = "grey", size = 0.6) +
  stat_halfeye() +
  facet_wrap("parameter", scales = "free_x") +
  xlab("Difference from normal speed condition (on probability/proportion scale)") + 
  ylab("   ") +
  scale_x_continuous(labels = scales::percent_format(1)) 

#pr2 + theme(axis.ticks.y = element_blank())

ciwidths <- comppars %>% 
  group_by(parameter, condition) %>% 
  mean_qi() %>% 
  mutate(width = .upper - .lower) %>% 
  summarise(mwdith = mean(width))

tmp <- pr2$data %>% 
  group_by(parameter) %>% 
  summarise(min = min(estimate), 
            max = max(estimate)) %>% 
  mutate(pwidth = max-min) %>% 
  mutate(mid = min + (max-min)/2) %>% 
  mutate(ciwdith = ciwidths$mwdith) %>% 
  mutate(newwidth = ciwdith * max(pwidth/ciwdith)) %>% 
  mutate(newmin = mid - newwidth/2,
         newmax = mid + newwidth/2)
tmpl <- tmp %>% 
  select(parameter, newmin, newmax) %>% 
  pivot_longer(cols = c(newmin, newmax), 
               names_to = "bounds", values_to = "estimate") %>% 
  mutate(condition = NA)
pr2 <- pr2 +
  geom_blank(data = tmpl)

pr2 + theme_bw(base_size = 17) + 
  theme(
    panel.grid.minor.x = element_blank(), 
    panel.grid.major.x = element_blank()
  )

ggsave(pr2  + theme_bw(base_size = 17) + theme(
    panel.grid.minor.x = element_blank(), 
    panel.grid.major.x = element_blank()
  ), filename = "figures/res_speed_diff.png", 
       width = 9.5, height = 4)
pr2use <- pr2  + theme_bw(base_size = 17) + theme(
    panel.grid.minor.x = element_blank(), 
    panel.grid.major.x = element_blank(),
    axis.ticks.y = element_blank()
  )
```

```{r}
ggsave(plot = cowplot::plot_grid(pr1use, pr2use, ncol = 1, 
                                 rel_heights = c(4.5, 4)), 
       filename = "figures/res_speed_both.png", 
       width = 9.5, height = 8.5)

```

## Difference Distributions with Possible Priors

The following plot shows all the difference distributions with overlayed density estimate (in black) and some possible prior distributions in colour (note again that the model did not actually include any priors). These priors are normal priors (who have a higher peak at 0 compared to Cauchy and t) with two different SDs.

```{r, out.width="100%", fig.width=8, fig.asp=0.3, warning=FALSE}
comppars %>% 
  ggplot() +
  geom_histogram(aes(estimate, after_stat(density)), binwidth = 0.001) +
  geom_density(aes(estimate)) +
  stat_function(fun = dnorm, n = 101, args = list(mean = 0, sd = c(0.05)), 
                mapping =  aes(color = "SD = 0.05")) +
  stat_function(fun = dnorm, n = 101, args = list(mean = 0, sd = c(0.1)), 
                mapping =  aes(color = "SD = 0.1")) +
  stat_function(fun = dnorm, n = 101, args = list(mean = 0, sd = c(0.25)), 
                mapping =  aes(color = "SD = 0.25")) +
  geom_vline(xintercept = 0, color = "grey") +
  facet_grid(rows = vars(condition), 
             cols = vars(parameter), scales = "free_x") + 
  guides(colour = guide_legend("prior width"))
```

Note that a prior with SD of .05 means that we expect with 95% probability that the largest effect we observe is `r scales::percent(plogis(0.05 *2) - 0.5, accuracy = 0.01)` on the response scale.

# Model 1 With PGSI

The covariate scores do not differ between conditions (evidence for the null).

```{r, warning=FALSE}
1/anovaBF(pgsi ~ exp_cond, part2, progress = FALSE)
```


The following table shows mean and SDs of the covariates per group.

```{r}
part2 %>% 
  group_by(exp_cond) %>% 
  summarise(across(c(pgsi), list(mean = mean, sd = sd)))
```


## Model with Main Effects

```{r, eval=TRUE, include=FALSE}
zoib_model2 <- bf(
  new_prop ~ exp_cond + pgsi_c,
  phi ~ 1,
  zipp ~ exp_cond + pgsi_c,
  coi ~ exp_cond + pgsi_c, 
  family = zoib2
)

# make_stancode(zoib_model, data = part2, stanvars = stanvars, prior = np)
# tmp <- make_standata(zoib_model, data = part2, stanvars = stanvars) 
# str(tmp)
# tmp$Y

tmp_model_filename <- "model_fits/model_zoib2.rda"
if (file.exists(tmp_model_filename)) {
  load(tmp_model_filename)
} else {
  mzoib2 <- brm(formula = zoib_model2, data = part2, 
                stanvars = stanvars, prior = np,
                iter = 26000, warmup = 1000, chains = 4)
  save(mzoib2, file = tmp_model_filename, compress = "xz")
}



```

```{r}
summary(mzoib2)
```

As a visual convergence check, we plot the density and trace plots for the four intercept parameters representing the normal speed condition or the overall mean (for `phi`).

```{r, fig.asp=1.2}
plot(mzoib2, pars = "Intercept")
```

We can also plot the three parameters showing the difference distribution of the slowed down condition from the normal speed condition. These differences are given on the logit scale.

```{r, fig.asp=1.0}
plot(mzoib2, pars = "_condSloweddown")
```



```{r, include=FALSE}
get_variables(mzoib2)
```

```{r, fig.width=6, out.width="100%"}
pp_check(mzoib2, type = "hist", binwidth = 0.025, nsamples = 11) +
  coord_cartesian(ylim = c(0, 350))
```



```{r}

lpars <- mzoib2 %>% 
  gather_draws(`b_.*`, regex = TRUE) %>% 
  filter(!(.variable %in% c("b_phi_Intercept"))) %>%
  filter(!str_detect(.variable, "pgsi|motives")) %>% 
  mutate(type = case_when(
    str_detect(.variable, "Intercept") ~ "Intercept",
    str_detect(.variable, "condSloweddown") ~ "slowed"
  )) %>%  
  mutate(parameter = case_when(
    str_detect(.variable, "zipp") ~ "g",
    str_detect(.variable, "coi") ~ "f",
    TRUE ~ "mu"
  ))
#unique(lpars$.variable)

condpars <- lpars %>% 
  pivot_wider(id_cols = c(.chain:.draw, parameter, parameter), 
              names_from = type, values_from = .value) %>% 
  mutate(Normal = Intercept, 
         SlowedDown = Intercept + slowed) %>% 
  select(-Intercept, -slowed) %>% 
  pivot_longer(cols = c(Normal, SlowedDown), 
               names_to = "condition", values_to = "estimate") %>% 
  mutate(estimate = plogis(estimate)) %>% 
  mutate(parameter = factor(
    x = parameter, 
    levels = c("g", "f", "mu"), 
    labels = par_labels))

# condpars %>% 
#   group_by(parameter, condition) %>% 
#   mean_qi()
```

```{r, out.width="100%", fig.width=6, fig.asp=0.55}
condpars %>% 
  ggplot(aes(x = estimate, y = condition)) +
  #stat_histintervalh(breaks = 40) +
  stat_halfeye() +
  facet_wrap("parameter", scales = "free_x") +
  xlab("Probability/Proportion") +
  scale_x_continuous(labels = scales::percent_format(1))
```


```{r, out.width="100%", fig.width=6, fig.asp=0.55}
comppars <- condpars %>% 
  group_by(parameter) %>% 
  compare_levels(estimate, by = condition) %>% 
  mutate(condition = factor(
    x = condition, labels = "          "
  ))
comppars %>% 
  group_by(parameter, condition) %>% 
  mean_qi()

comppars %>% 
  ggplot(aes(x = estimate, y = condition)) +
  geom_vline(xintercept = 0, color = "grey", size = 0.6) +
  stat_halfeye() +
  facet_wrap("parameter", scales = "free_x") +
  xlab("Difference from normal speed condition (on probability/proportion scale)") + 
  ylab("   ") +
  scale_x_continuous(labels = scales::percent_format(1)) 
```

### Plot of Relationships and BANOVA

The figure below provides an alternative visualisation of the relationships between covariates and betting behaviour. In particular, participants were categorized into one of three experimental betting behavior groups: participants who did not bet at all (“none”, 14% of participants), participants who bet some of their money (68% of participants), and participants who bet “all” of their money (18%). For both gambling scales we see a positive relationship between the betting behavior group and the gambling score. Participants who bet more have on average higher scores on the two gambling scales. 


```{r, eval=FALSE}
part2 %>% 
  summarise(gamble_at_all = 1 - mean(new_prop  == 0), 
            gamble_everything = mean(new_prop[new_prop != 0] == 1), 
            proportion_bet_rest = mean(new_prop[!(new_prop %in% c(0, 1))]))

part2 %>% 
  filter(!(new_prop %in% c(0, 1))) %>% 
  with(cor.test(new_prop, pgsi))

part2 %>% 
  filter(!(new_prop %in% c(1))) %>% 
  with(cor.test(new_prop, pgsi))


part2 %>% 
  filter(!(new_prop %in% c(0, 1))) %>% 
  ggplot(aes(x = new_prop, y = pgsi)) +
  geom_point(alpha = 0.2)

part2 %>% 
  #filter(!(new_prop %in% c(1))) %>% 
  ggplot(aes(x = new_prop, y = motives)) +
  geom_jitter(alpha = 0.2, width = 0.01, height = 0.2) +
  geom_smooth()

part2 %>% 
  #filter(!(new_prop %in% c(1))) %>% 
  ggplot(aes(x = new_prop, y = pgsi)) +
  geom_jitter(alpha = 0.2, width = 0.01, height = 0.2) +
  geom_smooth()
```


```{r}

part2 <- part2 %>% 
  mutate(gamble_cat = if_else(new_prop == 1, "all", if_else(new_prop == 0, "none", "some"))) %>% 
  mutate(gamble_cat = factor(gamble_cat, levels = c("none", "some", "all")))
#library("ggbeeswarm")

prop.table(table(part2$gamble_cat))

cvp1 <- part2 %>% 
  #filter(!(new_prop %in% c(0))) %>% 
  #ggplot(aes(x = interaction(exp_cond, gamble_cat), y = pgsi)) +
  ggplot(aes(x = gamble_cat, y = pgsi)) +
  geom_violin(draw_quantiles = c(0.25, 0.5, 0.75)) +
  #geom_quasirandom(alpha = 0.2) +
  stat_summary(fun.data = ~mean_se(., mult = 1.96)) +
  xlab("Proportion bet") + ylab("PGSI") +
  theme_bw(base_size = 15) + theme(
    panel.grid.minor.x = element_blank(), 
    panel.grid.major.x = element_blank()
  )

ggsave(filename = "figures/cov_plot.png", plot = cvp1, width = 3.5, height = 4)

cvp1

```

This is supported by Bayesian ANOVAs with Bayes factors of over 100 for the effect of betting behavior group on the PGSI scores. However, this effect was not moderated by gambling speed condition. In particular, there was evidence for the absence of both a main effect of gambling message condition and an interaction of gambling message condition with betting behaviour group for both gambling scale scores (Bayes factors for the null > 25).

```{r, eval=FALSE}
anovaBF(pgsi ~ gamble_cat, part2, progress = FALSE)
```

```{r, warning=FALSE}
bfa1 <- anovaBF(pgsi ~ gamble_cat*exp_cond, part2, progress = FALSE)
bfa1
bfa1[2]/bfa1

```


## Model With Interactions

```{r, eval=TRUE, include=FALSE}
zoib_model3 <- bf(
  new_prop ~ exp_cond * pgsi_c,
  phi ~ 1,
  zipp ~ exp_cond * pgsi_c,
  coi ~ exp_cond * pgsi_c, 
  family = zoib2
)

# make_stancode(zoib_model, data = part2, stanvars = stanvars, prior = np)
# tmp <- make_standata(zoib_model, data = part2, stanvars = stanvars) 
# str(tmp)
# tmp$Y

tmp_model_filename <- "model_fits/model_zoib3.rda"
if (file.exists(tmp_model_filename)) {
  load(tmp_model_filename)
} else {
  mzoib3 <- brm(formula = zoib_model3, data = part2, 
                stanvars = stanvars, prior = np,
                iter = 26000, warmup = 1000, chains = 4)
  save(mzoib3, file = tmp_model_filename, compress = "xz")
}
```

```{r}
summary(mzoib3)
```

As a visual convergence check, we plot the density and trace plots for the four intercept parameters representing the normal speed condition or the overall mean (for `phi`).

```{r, fig.asp=1.2}
plot(mzoib3, pars = "Intercept")
```

We can also plot the three parameters showing the difference distribution of the slowed down condition from the normal speed condition. These differences are given on the logit scale.

```{r, fig.asp=1.0}
plot(mzoib3, pars = "_condSloweddown$")
```


```{r, include=FALSE}
get_variables(mzoib3)
```

```{r, fig.width=6, out.width="100%"}
pp_check(mzoib3, type = "hist", binwidth = 0.025, nsamples = 11) +
  coord_cartesian(ylim = c(0, 350))
```



```{r}

lpars <- mzoib3 %>% 
  gather_draws(`b_.*`, regex = TRUE) %>% 
  filter(!(.variable %in% c("b_phi_Intercept"))) %>%
  filter(!str_detect(.variable, "pgsi|motives")) %>% 
  mutate(type = case_when(
    str_detect(.variable, "Intercept") ~ "Intercept",
    str_detect(.variable, "condSloweddown") ~ "slowed"
  )) %>%  
  mutate(parameter = case_when(
    str_detect(.variable, "zipp") ~ "g",
    str_detect(.variable, "coi") ~ "f",
    TRUE ~ "mu"
  ))
#unique(lpars$.variable)

condpars <- lpars %>% 
  pivot_wider(id_cols = c(.chain:.draw, parameter, parameter), 
              names_from = type, values_from = .value) %>% 
  mutate(Normal = Intercept, 
         SlowedDown = Intercept + slowed) %>% 
  select(-Intercept, -slowed) %>% 
  pivot_longer(cols = c(Normal, SlowedDown), 
               names_to = "condition", values_to = "estimate") %>% 
  mutate(estimate = plogis(estimate)) %>% 
  mutate(parameter = factor(
    x = parameter, 
    levels = c("g", "f", "mu"), 
    labels = par_labels))

# condpars %>% 
#   group_by(parameter, condition) %>% 
#   mean_qi()
```

```{r, out.width="100%", fig.width=6, fig.asp=0.55}
condpars %>% 
  ggplot(aes(x = estimate, y = condition)) +
  #stat_histintervalh(breaks = 40) +
  stat_halfeye() +
  facet_wrap("parameter", scales = "free_x") +
  xlab("Probability/Proportion") +
  scale_x_continuous(labels = scales::percent_format(1))
```


```{r, out.width="100%", fig.width=6, fig.asp=0.55}
comppars <- condpars %>% 
  group_by(parameter) %>% 
  compare_levels(estimate, by = condition) %>% 
  mutate(condition = factor(
    x = condition, labels = "          "
  ))
comppars %>% 
  group_by(parameter, condition) %>% 
  mean_qi()

comppars %>% 
  ggplot(aes(x = estimate, y = condition)) +
  geom_vline(xintercept = 0, color = "grey", size = 0.6) +
  stat_halfeye() +
  facet_wrap("parameter", scales = "free_x") +
  xlab("Difference from normal speed condition (on probability/proportion scale)") + 
  ylab("   ") +
  scale_x_continuous(labels = scales::percent_format(1)) 
```

# Model 2: Total Number of Spins

```{r}
part_nozero <- part2 %>% 
  filter(amount != 0)
```


```{r}
part2 %>% 
  ggplot(aes(bet_count)) +
  geom_bar()
```

```{r}


nb_mod1 <- bf(
  bet_count | trunc(lb = 1) ~ exp_cond,
  family = negbinomial
)

tmp_model_filename <- "model_fits/model2_v1.rda"
if (file.exists(tmp_model_filename)) {
  load(tmp_model_filename)
} else {
  mnb1 <- brm(formula = nb_mod1, data = part_nozero, 
                iter = 26000, warmup = 1000, chains = 4)
  save(mnb1, file = tmp_model_filename, compress = "xz")
}
```

```{r}
summary(mnb1)
```

```{r, fig.asp=0.9}
plot(mnb1)
```

The data seems to be well described by the model.

```{r, fig.width=6, out.width="100%", warning=FALSE}
pp_check(mnb1, type = "hist", binwidth = 0.025, nsamples = 11, ntrys = 100) 
```

When we zoom in (i.e., ignore data points above 50 for the plot), we can see the that the real and synthetic data match quite well.

```{r, fig.width=6, out.width="100%", warning=FALSE}
pp_check(mnb1, type = "hist", binwidth = 0.025, nsamples = 11, ntrys = 100) +
  coord_cartesian(xlim = c(0, 50))
```

```{r}
mnb1 %>% 
  emmeans("exp_cond") %>% 
  gather_emmeans_draws() %>% 
  mutate(mean = exp(.value)) %>% 
  ggplot(aes(y = exp_cond, x = mean)) +
  stat_halfeye() +
  ylab(ylabel) + xlab("Predicted mean number of spins")
```

```{r}
mnb1 %>% 
  emmeans("exp_cond") %>% 
  gather_emmeans_draws() %>% 
  mutate(mean = exp(.value)) %>% 
  median_qi(mean)
```


```{r}
mnb1 %>% 
  emmeans("exp_cond") %>% 
  gather_emmeans_draws() %>% 
  mutate(mean = exp(.value)) %>% 
  compare_levels(mean, exp_cond) %>% 
  ggplot(aes(x = mean)) +
  stat_halfeye() +
  labs(x = "Difference in mean number of spins", y = "") +
  theme(axis.ticks.y = element_blank(), 
        axis.text.y = element_blank(), 
        axis.title.y = element_blank())
```

# Model 2 With PGSI

## With Main Effects

```{r}
nb_mod2 <- bf(
  bet_count | trunc(lb = 1) ~ exp_cond + pgsi_c,
  family = negbinomial
)

tmp_model_filename <- "model_fits/model2_v2.rda"
if (file.exists(tmp_model_filename)) {
  load(tmp_model_filename)
} else {
  mnb2 <- brm(formula = nb_mod2, data = part_nozero, 
                iter = 26000, warmup = 1000, chains = 4, inits = "0")
  save(mnb2, file = tmp_model_filename, compress = "xz")
}
```

```{r}
summary(mnb2)
```

```{r, fig.asp=1.0}
plot(mnb2)
```


```{r}
mnb2 %>% 
  emmeans("exp_cond") %>% 
  gather_emmeans_draws() %>% 
  mutate(mean = exp(.value)) %>% 
  ggplot(aes(y = exp_cond, x = mean)) +
  stat_halfeye() +
  ylab(ylabel) + xlab("Predicted mean number of spins")
```

```{r}
mnb2 %>% 
  emmeans("exp_cond") %>% 
  gather_emmeans_draws() %>% 
  mutate(mean = exp(.value)) %>% 
  median_qi(mean)
```


```{r}
mnb2 %>% 
  emmeans("exp_cond") %>% 
  gather_emmeans_draws() %>% 
  mutate(mean = exp(.value)) %>% 
  compare_levels(mean, exp_cond) %>% 
  ggplot(aes(x = mean)) +
  stat_halfeye() +
  labs(x = "Difference in mean number of spins", y = "") +
  theme(axis.ticks.y = element_blank(), 
        axis.text.y = element_blank(), 
        axis.title.y = element_blank())
```

## With Interactions

```{r}
nb_mod3 <- bf(
  bet_count | trunc(lb = 1) ~ exp_cond * pgsi_c,
  family = negbinomial
)

tmp_model_filename <- "model_fits/model2_v3.rda"
if (file.exists(tmp_model_filename)) {
  load(tmp_model_filename)
} else {
  mnb3 <- brm(formula = nb_mod3, data = part_nozero, 
                iter = 26000, warmup = 1000, chains = 4, inits = "0")
  save(mnb3, file = tmp_model_filename, compress = "xz")
}
```

```{r}
summary(mnb3)
```

```{r, fig.asp=1.0}
plot(mnb3)
```


```{r}
mnb3 %>% 
  emmeans("exp_cond") %>% 
  gather_emmeans_draws() %>% 
  mutate(mean = exp(.value)) %>% 
  ggplot(aes(y = exp_cond, x = mean)) +
  stat_halfeye() +
  ylab(ylabel) + xlab("Predicted mean number of spins")
```

```{r}
mnb3 %>% 
  emmeans("exp_cond") %>% 
  gather_emmeans_draws() %>% 
  mutate(mean = exp(.value)) %>% 
  median_qi(mean)
```


```{r}
mnb3 %>% 
  emmeans("exp_cond") %>% 
  gather_emmeans_draws() %>% 
  mutate(mean = exp(.value)) %>% 
  compare_levels(mean, exp_cond) %>% 
  ggplot(aes(x = mean)) +
  stat_halfeye() +
  labs(x = "Difference in mean number of spins", y = "") +
  theme(axis.ticks.y = element_blank(), 
        axis.text.y = element_blank(), 
        axis.title.y = element_blank())
```


# Model 3: Average Bet Size

```{r}
part_nozero <- part_nozero %>% 
  mutate(average_scaled = average / 2)
```

The following shows the distribution of average bet sizes for those that bet at least once. The left plot shows the actual averages and the right plot the average scaled to the 0 to 1 range (i.e., after dividing by 2 as pre-registered). We can see that the distribution of actual averages is multi-modal with peaks at several prominent numbers: the first peak at the minimum possible bet size 0.1, the second peak at 0.5, the largest peak at the center at 1 and finally a large peak at the maximum of 2. As pre-registered we will analyse the data with a one-inflated beta-regression model.

```{r}
cowplot::plot_grid(
  part_nozero %>% 
  ggplot(aes(average)) +
  geom_histogram(bins = 50), 
  part_nozero %>% 
  ggplot(aes(average_scaled)) +
  geom_histogram(bins = 50), nrow = 1
)
```


```{r, eval=TRUE, include=FALSE}
oib <- custom_family(
  "oib", dpars = c("mu", "phi", "oi"),
  links = c("logit", "log", "logit"), lb = c(NA, 0, NA),
  type = "real"
)

stan_funs <- "
/* zero-one-inflated beta log-PDF of a single response 
   * Args: 
   *   y: response value 
   *   mu: mean parameter of the beta part
   *   phi: precision parameter of the beta part
   *   oi: one-inflation probability
   * Returns:  
   *   a scalar to be added to the log posterior 
   */ 
   real oib_lpdf(real y, real mu, real phi, real oi) {
     row_vector[2] shape = [mu * phi, (1 - mu) * phi]; 
     if (y == 1) {
       return bernoulli_lpmf(1 | oi);
     } else { 
       return bernoulli_lpmf(0 | oi) + beta_lpdf(y | shape[1], shape[2]);
     } 
   }
"
stanvars <- stanvar(scode = stan_funs, block = "functions")


oib_model <- bf(
  average_scaled ~ exp_cond,
  phi ~ 1,
  oi ~ exp_cond, 
  family = oib
)

np <- prior_string("target += logistic_lpdf(Intercept_oi | 0, 1)", check = FALSE)

# make_stancode(oib_model, data = part_nozero, stanvars = stanvars, prior = np)
# tmp <- make_standata(zoib_model, data = part2, stanvars = stanvars) 
# str(tmp)
# tmp$Y
tmp_model_filename <- "model_fits/model_oib.rda"
if (file.exists(tmp_model_filename)) {
  load(tmp_model_filename)
} else {
  moib <- brm(formula = oib_model, data = part_nozero, 
               stanvars = stanvars, prior = np, 
               iter = 26000, warmup = 1000, chains = 4) ## 100,000 post-warmup
save(moib, file = tmp_model_filename, compress = "xz")
}

```

```{r}
summary(moib)
```

```{r, fig.asp=1.0}
plot(moib)
```



```{r}
## we need to create a custom RNG function for this case
posterior_predict_oib <- function(i, prep, ...) {
  oi <- brms:::get_dpar(prep, "oi", i)
  mu <- brms:::get_dpar(prep, "mu", i = i)
  phi <- brms:::get_dpar(prep, "phi", i = i)
  hu <- runif(prep$nsamples, 0, 1)
  #one_or_zero <- runif(prep$nsamples, 0, 1)
  ifelse(hu < oi, 1, 
         rbeta(prep$nsamples, shape1 = mu * phi, shape2 = (1 - mu) * phi)
  )
}

```

When comparing the actual data with synthetic data from the model, we can see an obvious problem due to the use of prominent numbers. Whereas the peak at 1 is nicely recovered by the model, the other peaks are not. However, the overall shape of the synthetic data is at least similar to the shape of the observed data.

```{r, fig.width=6, out.width="100%"}
pp_check(moib, type = "hist", binwidth = 0.025, nsamples = 11) 
```

```{r}
post_moi_mu <- moib %>% 
  emmeans("exp_cond", dpar = "mu") %>% 
  gather_emmeans_draws() %>% 
  mutate(mu = plogis(.value)) %>% 
  select(-.value)

post_moi_oi <- moib %>% 
  emmeans::emmeans("exp_cond", dpar = "oi") %>% 
  gather_emmeans_draws() %>% 
  mutate(oi = plogis(.value))  %>% 
  select(-.value) 

post_both <- left_join(post_moi_mu, post_moi_oi) %>%
  pivot_longer(c(mu, oi), names_to = "parameter", values_to = "value") %>%
  mutate(parameter = factor(parameter, levels = c("mu", "oi"), 
                            labels = c("Mean bet size", "Bet everything"))) 
ggplot(post_both, aes(x = value, y = exp_cond)) +
  stat_halfeye() +
  facet_wrap("parameter", scales = "free_x") +
  ylab(ylabel) + xlab("Probability/Proportion")
```

Difference distribution:

```{r}
post_both %>%
  group_by(parameter) %>% 
  compare_levels(value, exp_cond) %>% 
  ggplot(aes(value)) +
  stat_halfeye() +
  facet_wrap("parameter", scales = "free_x") +
  labs(x = "Differences in average bet sizes", y = "") +
  theme(axis.ticks.y = element_blank(), 
        axis.text.y = element_blank(), 
        axis.title.y = element_blank())
```

# Model 3 with PGSI

## With Main Effect

```{r, eval=TRUE, include=FALSE}

oib_model2 <- bf(
  average_scaled ~ exp_cond + pgsi_c,
  phi ~ 1,
  oi ~ exp_cond, 
  family = oib
)

tmp_model_filename <- "model_fits/model_oib_v2.rda"
if (file.exists(tmp_model_filename)) {
  load(tmp_model_filename)
} else {
  moib2 <- brm(formula = oib_model2, data = part_nozero, 
               stanvars = stanvars, prior = np, 
               iter = 26000, warmup = 1000, chains = 4) ## 100,000 post-warmup
save(moib2, file = tmp_model_filename, compress = "xz")
}

```

```{r}
summary(moib2)
```

```{r, fig.asp=1.0}
plot(moib2, pars = "Sloweddown|pgsi")
```

```{r}
post_moi_mu <- moib2 %>% 
  emmeans("exp_cond", dpar = "mu") %>% 
  gather_emmeans_draws() %>% 
  mutate(mu = plogis(.value)) %>% 
  select(-.value)

post_moi_oi <- moib2 %>% 
  emmeans::emmeans("exp_cond", dpar = "oi") %>% 
  gather_emmeans_draws() %>% 
  mutate(oi = plogis(.value))  %>% 
  select(-.value) 

post_both <- left_join(post_moi_mu, post_moi_oi) %>%
  pivot_longer(c(mu, oi), names_to = "parameter", values_to = "value") %>%
  mutate(parameter = factor(parameter, levels = c("mu", "oi"), 
                            labels = c("Mean bet size", "Bet everything"))) 
ggplot(post_both, aes(x = value, y = exp_cond)) +
  stat_halfeye() +
  facet_wrap("parameter", scales = "free_x") +
  ylab(ylabel) + xlab("Probability/Proportion")
```

Difference distribution:

```{r}
post_both %>%
  group_by(parameter) %>% 
  compare_levels(value, exp_cond) %>% 
  ggplot(aes(value)) +
  stat_halfeye() +
  facet_wrap("parameter", scales = "free_x") +
  labs(x = "Differences in average bet sizes", y = "") +
  theme(axis.ticks.y = element_blank(), 
        axis.text.y = element_blank(), 
        axis.title.y = element_blank())
```

## With Interaction

```{r, eval=TRUE, include=FALSE}

oib_model3 <- bf(
  average_scaled ~ exp_cond * pgsi_c,
  phi ~ 1,
  oi ~ exp_cond, 
  family = oib
)

tmp_model_filename <- "model_fits/model_oib_v3.rda"
if (file.exists(tmp_model_filename)) {
  load(tmp_model_filename)
} else {
  moib3 <- brm(formula = oib_model3, data = part_nozero, 
               stanvars = stanvars, prior = np, 
               iter = 26000, warmup = 1000, chains = 4) ## 100,000 post-warmup
save(moib3, file = tmp_model_filename, compress = "xz")
}

```

```{r}
summary(moib3)
```

```{r, fig.asp=1.0}
plot(moib3, pars = "Sloweddown|pgsi")
```

```{r, message=FALSE}
post_moi_mu <- moib3 %>% 
  emmeans("exp_cond", dpar = "mu") %>% 
  gather_emmeans_draws() %>% 
  mutate(mu = plogis(.value)) %>% 
  select(-.value)

post_moi_oi <- moib3 %>% 
  emmeans::emmeans("exp_cond", dpar = "oi") %>% 
  gather_emmeans_draws() %>% 
  mutate(oi = plogis(.value))  %>% 
  select(-.value) 

post_both <- left_join(post_moi_mu, post_moi_oi) %>%
  pivot_longer(c(mu, oi), names_to = "parameter", values_to = "value") %>%
  mutate(parameter = factor(parameter, levels = c("mu", "oi"), 
                            labels = c("Mean bet size", "Bet everything"))) 
ggplot(post_both, aes(x = value, y = exp_cond)) +
  stat_halfeye() +
  facet_wrap("parameter", scales = "free_x") +
  ylab(ylabel) + xlab("Probability/Proportion")
```

Difference distribution:

```{r}
post_both %>%
  group_by(parameter) %>% 
  compare_levels(value, exp_cond) %>% 
  ggplot(aes(value)) +
  stat_halfeye() +
  facet_wrap("parameter", scales = "free_x") +
  labs(x = "Differences in average bet sizes", y = "") +
  theme(axis.ticks.y = element_blank(), 
        axis.text.y = element_blank(), 
        axis.title.y = element_blank())
```


# Riskiness of Bets

Do either of the spin speeds affect the riskiness of bets chosen in the roulette? Different bets come with different potential payoffs in roulette. If £1 is bet, then this can provide a total payoff of between £2 (e.g., bets on red or black) and £36 (bets on a single number). These numbers, 2 and 36, are the decimal odds for these two bets, representing the total potential payoff Additionally, gamblers can place multiple bets per spin, (e.g., betting £0.50 on red and £0.50 on 7). Since the number 7 is a red colour, the bet on 7 can only win if the first bet on red also wins. Multiple bets per spin can either be placed in a way that accentuates risk, as in this example, or in a way that hedges risk (for example, a bet on black added to the bet on 7, or betting on reds and blacks together). The purpose of this exploratory analysis is to see if either warning label affects risk taking.
 
Following our pre-registration we have measures the amount of risk taken by looking at the variance of the decimal odds for each number in the roulette table. It will measure the concentration of the bet, with more risk represented by more concentrated bets (e.g., betting on a single number), and lower risk represented by more spread bets (e.g., betting on reds, betting on evens). For example, if a participant places a bet on number 7, the decimal odds for that number is 36 (36 times the amount bet if the roulette stops on the number 7). If a participant places a bet on red, then every red number on the table gets a decimal odd of 2 (the participant will win twice the amount bet if the roulette stops on any red number). In order to calculate the proposed risk variable, we will also assign decimal odds of zero for the numbers in which participants did not bet (since they win zero times the amount bet if the roulette stops on those numbers). In the two examples above, every number except 7 will be assigned zero, and every non-red number will be assigned zero, respectively. The risk variable will be the variance of the array of decimal odds for every number on the roulette table, taking into account the bets the participant has placed, and including zeroes for non-winning numbers. The value of this risk variable can range between 0.03 and 35.03. The value cannot be zero in our task because participants are not able to bet the same amount across all numbers including the zero due to the limitations in the values of the available tokens and total bet amounts. Higher numbers indicate more risk taking (with the highest, 35.03, associated with concentrating the bet on a single number). Lower numbers indicate lower risk taking (with the lowest, 0.03, associated with spreading the bet across every red and black number, excluding zero).

The following plot shows the distribution of this variable after aggregating within participants. The left plot shows the original variable on the scale from 0.03 to 35.03. The middle plot shows the variable after dividing by 36 so that the variable ranges from just above 0 to just below 1. The right plot shows the variable after subtracting 0.03 and dividing by 35 so that the smallest value is mapped to 0 and the largest value is mapped to 1. 


```{r, out.width="100%", fig.width=6, fig.asp=0.5}
bets2 <- left_join(bets, select(part2, ppt_id, exp_cond), by = "ppt_id") %>% 
  filter(ppt_id %in% part2$ppt_id) %>% 
  mutate(average_odds2 = average_odds / 36) %>% 
  mutate(average_odds3 = (average_odds - 0.03) / 35)

bets_av <- bets2 %>% 
  group_by(exp_cond, ppt_id) %>% 
  summarise(across(starts_with("average_odds"), mean)) %>% 
  ungroup()

# bets2 %>% 
#   summarise(mean(average_odds3 == 0),
#             mean(average_odds3 == 1))
# 
# bets_av %>% 
#   summarise(mean(average_odds3 == 0),
#             mean(average_odds3 == 1))

cowplot::plot_grid(
  bets_av %>% 
  ggplot(aes(average_odds)) +
  geom_histogram(bins = 50), 
  bets_av %>% 
  ggplot(aes(average_odds2)) +
  geom_histogram(bins = 50), 
  bets_av %>% 
  ggplot(aes(average_odds3)) +
  geom_histogram(bins = 50), nrow = 1
)

  #facet_wrap("exp_cond")
```

## Beta Regression

We first analyse the data with a beta-regression model. For this, the data needs to be between 0 and 1, but exclude exactly 0 and 1. Consequently, we use the transformation shown above in the middle panel. As before, we fit the data and allow for an effect of gambling message. 


```{r, eval=TRUE, include=FALSE}
risk_frml2 <- bf(
  average_odds2 ~ exp_cond,
  phi ~ 1
)

# risk_frml <- bf(
#   average_odds3 ~ exp_cond + (1 |s| ppt_id),
#   phi ~ 1,
#   zoi ~ exp_cond + (1 |s| ppt_id),
#   coi ~ exp_cond + (1 |s| ppt_id)
# )

# zoib_model <- bf(
#   new_prop ~ exp_cond,
#   phi ~ 1,
#   zipp ~ exp_cond,
#   coi ~ exp_cond, 
#   family = zoib2
# )

# make_stancode(zoib_model, data = part2, stanvars = stanvars, prior = np)
# tmp <- make_standata(zoib_model, data = part2, stanvars = stanvars) 
# str(tmp)
# tmp$Y
tmp_model_filename <- "model_fits/model_risk2.rda"
if (file.exists(tmp_model_filename)) {
  load(tmp_model_filename)
} else {
  mrisk2 <- brm(formula = risk_frml2, data = bets_av, family = Beta())
  save(mrisk2, file = tmp_model_filename, compress = "xz")
}

```

The model does not show any obvious problems. In addition, we can see that the 95%-CIs for the slowed down effect includes 0.

```{r}
summary(mrisk2)
```

```{r, fig.asp=1.0}
plot(mrisk2)
```

The posterior predictive distribution shows some problems, but at least the shape of the synthetic data is similar to the distribution of the actual data (posterior predictive distributions of a model assuming a Gaussian response distribution was considerably worse and is therefore not included here).

```{r, fig.width=6, out.width="100%"}
pp_check(mrisk2, type = "hist", binwidth = 0.05, nsamples = 11)
```

The following tables show the average riskiness per condition and differences from the no warning message group on the fitted scale, i.e., the (0, 1) scale used for the beta regression.

```{r, warning=FALSE}

lpars <- mrisk2 %>% 
  gather_draws(`b_.*`, regex = TRUE) %>% 
  filter(.variable != "b_phi_Intercept") %>% 
  mutate(type = case_when(
    str_detect(.variable, "Intercept") ~ "Intercept",
    str_detect(.variable, "condSloweddown") ~ "slowed"
  )) 

condpars <- lpars %>% 
  pivot_wider(id_cols = c(.chain:.draw), 
              names_from = type, values_from = .value) %>% 
 mutate(Normal = Intercept, 
         SlowedDown = Intercept + slowed) %>% 
  select(-Intercept, -slowed) %>% 
  pivot_longer(cols = c(Normal, SlowedDown), 
               names_to = "condition", values_to = "estimate") %>% 
  mutate(estimate = plogis(estimate))


condpars %>% 
  group_by(condition) %>% 
  mean_qi()
comppars <- condpars %>% 
  compare_levels(estimate, by = condition) %>% 
  mutate(condition = factor(condition))

comppars %>% 
  group_by(condition) %>% 
  mean_qi()

```


# Session Info

```{r}
options(width = 100)
sessionInfo()
```


